# llm-reasoning-papers

## Train LLM to reason 
1. **[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via
Reinforcement Learning](https://arxiv.org/pdf/2501.12948)**
   <details>
     <summary>TL;DR</summary>
     Making LLM reason through pure RL (open-source) 
   </details>

2. **[DRT: Deep Reasoning Translation via Long Chain-of-Thought](https://arxiv.org/pdf/2412.17498)**
   <details>
     <summary>TL;DR</summary>
     Agentic + CoT for Machine Translation
   </details>
3. **[Scaling up Test-Time Compute with Latent Reasoning:
A Recurrent Depth Approach](https://arxiv.org/pdf/2502.05171)**
   <details>
     <summary>TL;DR</summary>
     Recurrent reasoning in latent space as compared to using tokens.
   </details>
4. **[On the Emergence of Thinking in LLMs I:
Searching for the Right Intuition](https://arxiv.org/pdf/2502.06773)**
   <details>
     <summary>TL;DR</summary>
     Propose RLSP (Reinforcement Learning via Self-Play) as a framework to understand and build large reasoning models.
   </details>
4. **[Competitive Programming with Large Reasoning Models
](https://arxiv.org/abs/2502.06807)**
   <details>
     <summary>TL;DR</summary>
    Demonstrates that competitive coding scaling test time computing leads to better performance than hand-crafted features used to choose a particular solution.
   </details>

5. **[Sky-T1-32B: Data-Efficient Training for Large Reasoning Models](https://github.com/NovaSky-AI/SkyThought)**
   <details>
     <summary>TL;DR</summary>
     Shows that Long Chain-of-Thought (Long CoT) reasoning can be efficiently learned through supervised fine-tuning (SFT) and LoRA with just 17k samples, significantly improving performance on math and coding benchmarks.
   </details>

## Efficient Reasoning methods 
1. **[s1: Simple test-time scaling](https://arxiv.org/pdf/2501.19393v2)**
   <details>
     <summary>TL;DR</summary>
     Post-training using SFT on 1000 samples leads to a reasoning model similar to o1 
   </details>
## Vision/Multimodal Reasoning 
1. **[Imagine while Reasoning in Space:
Multimodal Visualization-of-Thought](https://arxiv.org/pdf/2501.07542)**
   <details>
     <summary>TL;DR</summary>
    Proposes Multimodal Visualization-of-Thought (MVoT) having an intermediate thinking stack composed of vision and language.   
   </details>
## Related Blogs
1. **[Reinforcement Learning with Verifiable Rewards](https://vinija.ai/concepts/RFT)**
   <details>
     <summary>TL;DR</summary>
     Introduction to RLVR 
   </details>


